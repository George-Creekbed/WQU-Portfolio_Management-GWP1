{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "## a. Define each of the 5 factors in the Fama-French 5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. For each factor, explain how it helps to explain returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "Download daily data from [this site](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) for a timeframe of 3 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we include all Python package that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import cvxpy as cp\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 9)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Import, structure, and graph the daily factor returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the ``csv`` file into a ``pandas DataFrame``.\n",
    "\n",
    "We set our timeframe of observation on the 3-year period that goes from March 1st, 2021 to February 29th, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv to pd.DataFrame\n",
    "path = \"F-F_Research_Data_5_Factors_2x3_daily.csv\" # relative path of csv file\n",
    "df_daily = pd.read_csv(path, header=2, date_format='%Y%m%d', parse_dates=True)#, skipfooter=0, engine=\"python\")\n",
    "\n",
    "# adapt 1st column to date format in python\n",
    "df_daily['Unnamed: 0'] = df_daily['Unnamed: 0'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d'))\n",
    "df_daily = df_daily.rename(columns={'Unnamed: 0': 'Date'})\n",
    "df_daily = df_daily.set_index('Date')\n",
    "\n",
    "# 3-year timeframe chosen\n",
    "start = date(2021, 3, 1)\n",
    "end = date(2024, 2, 29)\n",
    "df_daily = df_daily[start: end]\n",
    "\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series for each of the Fama-French (FF) factors in the timeframe considered are visualised below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph time series of factors\n",
    "df_daily.plot(subplots=True, title=\"Time series of factors\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe add histograms of distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional statistics regarding the time series of the 5 FF factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Collect and compute correlations of the changes in the factor returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary to the analysis of their returns, we inspect the daily correlations of the five factors themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_corr = df_daily.corr()\n",
    "df_daily_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_daily_corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that almost all of the factors, with the exception of the risk free rate RF, are heavily correlated, either positively or negatively.\n",
    "\n",
    "The daily factor returns, expressed as percent change with respect to the previous day, are derived as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns = df_daily.pct_change()\n",
    "# drop nan values from 1st line, and set 0/0 divisions to 0.0\n",
    "factor_returns = factor_returns.drop(start)\n",
    "factor_returns = factor_returns.where(factor_returns.notna(), 0.0)\n",
    "\n",
    "factor_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, \n",
    "\n",
    "- we dropped `NaN` values stemming from the first line (that lacks previous reference data) of the DataFrame, and \n",
    "- we replaced $\\frac{0}{0} = \\text{NaN}$, mainly appearing in the RF column when both previous and current daily entries are $= 0$, with $0$.\n",
    "\n",
    "Plotting the time series of returns yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns.plot(subplots=True, title=\"Time series of daily factor returns\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the series look fairly stationary.\n",
    "\n",
    "One can however observe imperfections in most of the graphs, in the form of gaps within plottings (for instance, in the plot for RF at about 2022-04).\n",
    "These are $- \\infty$ or $\\infty$ values in the time series originated from dividing from a previous daily value of $0$.\n",
    "We correct this by setting infinity values to an arbitrarily large number, $\\pm 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set infinity points to + or - 1000\n",
    "factor_returns = factor_returns.where(factor_returns != np.inf, 1000.0)\n",
    "factor_returns = factor_returns.where(factor_returns != -np.inf, -1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally compute the correlation matrix between the five factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_corr = factor_returns.corr()\n",
    "factor_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(factor_corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series of first differences of factor returns are uncorrelated, with just one exception of weak correlation between RMW and CMA.\n",
    "\n",
    "However, for subsequent steps we will rely on the FF3/5 factors themselves, not on their first differences. \n",
    "This is because the latter do not lend themselves to be easily interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Collect economic data of your choice during that 3-year period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proxy for risk-free rate, we download from *Yahoo! Finance* data tracking the ^IRX index, which is based on yields from the 13-week US Treasury bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbill_13w = yf.download(\"^IRX\", start, end)\n",
    "tbill_13w = pd.DataFrame(tbill_13w[\"Adj Close\"])\n",
    "tbill_13w = tbill_13w.rename(columns={'Adj Close': '^IRX'})\n",
    "tbill_13w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As could be expected, the graph below shows that this interest rates index increases in value following worldwide inflation due to \n",
    "- disruptions in global production and supply of goods and services after the Covid pandemic, and \n",
    "- sanctions to Russia which increased costs for raw materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"2021-24 evolution of ^IRX index, which is based on 13-week Treasury bills\"\n",
    "tbill_13w.plot(figsize=(16, 5), title=title, ylabel=\"$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "Find the betas of factors in the Fama-French 3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Run both Least Squares and robust regressions on the data, and describe the train-test split.\n",
    "\n",
    "### Least Squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, as a first step we add the 13-weeks Treasury bill index ^IRX to the ```pandas``` DataFrame of FF3 factors for the timeframe considered, then we plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily['IRX'] = tbill_13w['^IRX']\n",
    "#df_daily['^IRX first diff'] = tbill_13w['^IRX'].pct_change()\n",
    "df_daily = df_daily.dropna()\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw scatterplot\n",
    "sns.pairplot(df_daily, vars=['IRX', 'Mkt-RF', 'SMB', 'HML'], height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots above show graphically how the FF3 factors are correlated among each other, but we can anticipate the dependent variable ^IRX, the Treasury bill rates is largely uncorrelated, i.e. independent from them.\n",
    "\n",
    "Histograms on the main diagonal of the pairplot show distributions of the variables over the selected 3-year timeframe.\n",
    "The distribution histogram for ^IRX shows that the dependent variable in our incoming analysis is clearly not normally distributed.\n",
    "\n",
    "Thus from the correlation and histogram plots above, we have just learned that \n",
    "- the dependent and independent variables of the incoming linear regression analysis are not in a linear relationship with each other\n",
    "- the three factors adopted as independent variables are fairly correlated with one another\n",
    "- the dependent variable ^IRX is not normally distributed.\n",
    "\n",
    "These observations lead us to anticipate that the linear regression of ^IRX from the three factors of the Fama-French model will not be successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.25\n",
    "test_set = int(test_ratio * len(df_daily))  # Number of observations in the test sample\n",
    "train_set = len(df_daily) - test_set  # observations in the train sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: splits the database into train and test data\n",
    "def trainTestSplit(df, ts):\n",
    "    Xdf, ydf = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    X = Xdf.astype(\"float32\")\n",
    "    y = ydf.astype(\"float32\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=ts, shuffle=False\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_FF3 = df_daily[['Mkt-RF', 'SMB', 'HML', 'IRX']]\n",
    "X_train, X_test, Y_train, Y_test = trainTestSplit(factors_FF3, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the linear regression model ^IRX = f(FF3)\n",
    "X_train = sm.add_constant(X_train)\n",
    "linear_regr_3_factors_model = sm.OLS(Y_train, X_train)\n",
    "linear_regr_3_factors = linear_regr_3_factors_model.fit()\n",
    "train_params = linear_regr_3_factors.params\n",
    "linear_regr_3_factors.summary()\n",
    "# Mkt-RF,SMB,HML,RMW,CMA,RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regr_FF3_train = linear_regr_3_factors_model.predict(train_params)#, exog=Y_test)\n",
    "training_time = df_daily.index[:train_set]\n",
    "print(len(training_time), len(linear_regr_FF3_train), len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_predictions_FF3 = pd.DataFrame(\n",
    "    {\"Date\": training_time, \"Predictions\": linear_regr_FF3_train, \"True values\": Y_train}\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "df_training_predictions_FF3.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='Predictions', ax=ax)\n",
    "df_training_predictions_FF3.plot.scatter(x=\"Date\", y=\"True values\", c='tab:orange', label='True values', ax=ax)\n",
    "# time_on_x = df_training_predictions_FF3[\"Date\"] - df_training_predictions_FF3[\"Date\"].min()#.apply(lambda x: x.date()))#.astype(np.int64)\n",
    "# time_on_x = time_on_x.astype(np.int64) * 1E-12\n",
    "# (a, b) = np.polyfit(time_on_x, df_training_predictions_FF3[\"Predictions\"], 1)\n",
    "# plt.plot(time_on_x, a * time_on_x + b, label=\"best regression fit line\")\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.legend()\n",
    "plt.title(\"FF3, Training period: True vs Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regr_FF3_test = linear_regr_3_factors.predict(sm.add_constant(X_test, has_constant='add')) # added alpha coefficient\n",
    "testing_time = df_daily.index[train_set:]\n",
    "print(len(testing_time), len(linear_regr_FF3_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_predictions_FF3 = pd.DataFrame(\n",
    "    {\"Date\": testing_time, \"Predictions\": linear_regr_FF3_test, \"True values\": Y_test}\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "df_testing_predictions_FF3.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='Predictions', ax=ax)\n",
    "df_testing_predictions_FF3.plot.scatter(x=\"Date\", y=\"True values\", c='tab:orange', label='True values', ax=ax)\n",
    "plt.legend()\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.title(\"FF3, Testing period: True vs Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF3_LS_predictions = pd.concat([df_training_predictions_FF3, df_testing_predictions_FF3],\n",
    "    keys=['training', 'testing'],\n",
    "    ignore_index=True                          \n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "FF3_LS_predictions.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='Predictions', ax=ax)\n",
    "FF3_LS_predictions.plot.scatter(x=\"Date\", y=\"True values\", c='tab:orange', label='True values', ax=ax)\n",
    "plt.vlines(x=df_testing_predictions_FF3[\"Date\"].iloc[0], \n",
    "           ymin=FF3_LS_predictions[\"True values\"].min(), \n",
    "           ymax=FF3_LS_predictions[\"True values\"].max(), \n",
    "           colors=\"r\", \n",
    "           linestyles=\"dashed\",\n",
    "           label=\"Train-Test split\")\n",
    "plt.legend()\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.title(\"FF3, Training + Testing period: True vs Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ordinary least squares (OLS) linear regression of ^IRX from the FF3 factors yields high to very high *p*-values for the estimates of the coefficients $\\beta_i$, $i= 1,\\, 2, \\, 3$.\n",
    "This is true not only inside the testing period, but for the training period as well.\n",
    "\n",
    "This means that none of the factors can explain the variation of the dependent variable ^IRX.\n",
    "Another indication that the OLS regression just performed is not significant is given by the very low value of the $R^2$ statistic which is practically $\\sim 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, for this model we plot the partial regression lines of the dependent variable, ^IDX, against each of the three independent variables of the FF3 model.\n",
    "The lines are obtained by plotting residuals of ^IRX against residuals of each factor, after having removed the effect of the after factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_partregress_grid(linear_regr_3_factors, exog_idx=[1,2,3], fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now regress the ^IRX index against the FF3 factors using a robust regression model(*M-Estimation*), in order to gain a better appreciation of the influence of outlying data points over the analysis.\n",
    "\n",
    "We will leave the train-test split of the dataset at a 75/25 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = trainTestSplit(factors_FF3, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the robust regression model ^IRX = Huber(FF3)\n",
    "X_train = sm.add_constant(X_train)\n",
    "robust_norm = [sm.robust.norms.HuberT(), sm.robust.norms.TukeyBiweight()]\n",
    "robust_regr_3_factors_model = []\n",
    "robust_regr_3_factors = []\n",
    "robust_train_params = []\n",
    "for i in range(len(robust_norm)):\n",
    "    robust_regr_3_factors_model.append(sm.RLM(endog=Y_train, exog=X_train, M=robust_norm[i]))\n",
    "    robust_regr_3_factors.append(robust_regr_3_factors_model[i].fit())\n",
    "    robust_train_params.append(robust_regr_3_factors[i].params)\n",
    "    print(robust_regr_3_factors[i].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Huber parameters:\\n\",robust_train_params[0])\n",
    "print(\"Bisquare parameters:\\n\",robust_train_params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_regr_FF3_train = robust_regr_3_factors_model[0].predict(robust_train_params[0])\n",
    "#training_time = df_daily.index[:train_set]\n",
    "print(len(training_time), len(robust_regr_FF3_train[0]), len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_predictions_robust_FF3 = pd.DataFrame(\n",
    "    {\"Date\": training_time, \"Predictions\": robust_regr_FF3_train[0], \"True values\": Y_train}\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "df_training_predictions_robust_FF3.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='Predictions', ax=ax)\n",
    "df_training_predictions_robust_FF3.plot.scatter(x=\"Date\", y=\"True values\", c='tab:orange', label='True values', ax=ax)\n",
    "# time_on_x = df_training_predictions_FF3[\"Date\"] - df_training_predictions_FF3[\"Date\"].min()#.apply(lambda x: x.date()))#.astype(np.int64)\n",
    "# time_on_x = time_on_x.astype(np.int64) * 1E-12\n",
    "# (a, b) = np.polyfit(time_on_x, df_training_predictions_FF3[\"Predictions\"], 1)\n",
    "# plt.plot(time_on_x, a * time_on_x + b, label=\"best regression fit line\")\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.legend()\n",
    "plt.title(\"FF3, robust regression, training period: True vs Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_regr_FF3_test = robust_regr_3_factors[0].predict(sm.add_constant(X_test, has_constant='add')) # added alpha coefficient\n",
    "#testing_time = df_daily.index[train_set:]\n",
    "print(len(testing_time), len(robust_regr_FF3_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_predictions_robust_FF3 = pd.DataFrame(\n",
    "    {\"Date\": testing_time, \"Predictions\": robust_regr_FF3_test, \"True values\": Y_test}\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "df_testing_predictions_robust_FF3.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='Predictions', ax=ax)\n",
    "df_testing_predictions_robust_FF3.plot.scatter(x=\"Date\", y=\"True values\", c='tab:orange', label='True values', ax=ax)\n",
    "plt.legend()\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.title(\"FF3, robust regression, testing period: True vs Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF3_robust_predictions = pd.concat([df_training_predictions_robust_FF3, df_testing_predictions_robust_FF3],\n",
    "    keys=['training', 'testing'],\n",
    "    ignore_index=True                          \n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "FF3_robust_predictions.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='Predictions', ax=ax)\n",
    "FF3_robust_predictions.plot.scatter(x=\"Date\", y=\"True values\", c='tab:orange', label='True values', ax=ax)\n",
    "plt.vlines(x=df_testing_predictions_robust_FF3[\"Date\"].iloc[0], \n",
    "           ymin=FF3_robust_predictions[\"True values\"].min(), \n",
    "           ymax=FF3_robust_predictions[\"True values\"].max(), \n",
    "           colors=\"r\", \n",
    "           linestyles=\"dashed\",\n",
    "           label=\"Train-Test split\"\n",
    ")\n",
    "plt.legend()\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.title(\"FF3, robust regression, Training + Testing period: True vs Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "FF3_LS_predictions.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:blue', label='OLS Predictions', ax=ax)\n",
    "FF3_robust_predictions.plot.scatter(x=\"Date\", y=\"Predictions\", c='tab:orange', label='Robust Predictions', ax=ax)\n",
    "plt.vlines(x=df_testing_predictions_robust_FF3[\"Date\"].iloc[0], \n",
    "           ymin=FF3_robust_predictions[\"Predictions\"].min(), \n",
    "           ymax=FF3_robust_predictions[\"Predictions\"].max(), \n",
    "           colors=\"r\", \n",
    "           linestyles=\"dashed\",\n",
    "           label=\"Train-Test split\"\n",
    ")\n",
    "plt.legend()\n",
    "ax.set_xlabel('Dates')\n",
    "ax.set_ylabel(\"IRX values\")\n",
    "plt.title(\"FF3, Training + Testing period: OLS vs Robust (Huber) Regression Predicted returns for IRX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shown above - comparing regressions yielded by the ordinary least squares method against the robust Huber method - reveals results that are almost identical.\n",
    "Robust regression mainly dampens the deleterious effects of outliers, so the above results exclude that the low quality of the regression is due to the impact of outliers.\n",
    "\n",
    "But it was already evident that the bad fitting of the analysis is due to the short rates tracked by ^IRX being independent from the Fama-French factors, which is tailored to track market data, not macroeconomic ones.\n",
    "\n",
    "Below, for this model we plot the partial regression lines of the dependent variable, ^IDX, against each of the three independent variables of the FF3 model.\n",
    "The lines are obtained by plotting residuals of ^IRX against residuals of each factor, after having removed the effect of the after factors.\n",
    "\n",
    "The first figure below depicts partial regressions based on the Huber norm, while the second picture is based on the bisquare norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_partregress_grid(robust_regr_3_factors[0], exog_idx=[1,2,3], fig=fig)\n",
    "plt.suptitle(\"Huber: Partial Regression Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_partregress_grid(robust_regr_3_factors[1], exog_idx=[1,2,3], fig=fig)\n",
    "plt.suptitle(\"Bisquare: Partial Regression Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Provide summaries of coefficients and metrics for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a summary of the analysis below.\n",
    "\n",
    "Refer to the equation below for naming parameters in the regression:\n",
    "\n",
    "$$\n",
    "\\text{IRX} = \\alpha + \\beta_0 \\, (\\text{Mkt-RF}) + \\beta_1 \\, (\\text{SMB}) + \\beta_2 \\, (\\text{HML})\n",
    "$$\n",
    "\n",
    "Summary of coefficients, with associated $p$-value in subsequent row.\n",
    "\n",
    "| Model            | $\\boldsymbol{\\alpha}$    | $\\boldsymbol{\\beta_0}$    | $\\boldsymbol{\\beta_1}$    | $\\boldsymbol{\\beta_2}$   |\n",
    "| ---------------- | ----------- |  ----------- | ------------ | ----------- |\n",
    "| OLS              |   1.7562    |   -0.0516    |    0.0466    |   -0.1565   | \n",
    "| OLS $p$-value    |   0.000    |    0.481    |    0.708     |    0.048   |\n",
    "| Robust Huber     |   1.7554    |   -0.0522    |    0.0457    |   -0.1565   | \n",
    "| Huber $p$-value  |   0.000    |   -0.479    |    0.716     |   0.048   | \n",
    "| Robust Bisquared |   1.6869    |   -0.0560    |    0.0530    |   -0.1612   |\n",
    "| Bisqu. $p$-value |   0.000    |   -0.482    |    0.695     |   0.06   |\n",
    "\n",
    "We can appreciate from the table above that for all models, only a weakly significant dependence of IRX from factor HML can be theorised. \n",
    "\n",
    "Summary of metrics for the OLS regression (values for the robust regressions are probably equal or very close, which would presumably be why no statistical metrics are available for them inside the ```statsmodels``` package).\n",
    "\n",
    "| Model            | $\\boldsymbol{R^2}$    | $\\bf{\\text{adj }} \\boldsymbol{R^2}$    | $\\bf{Jarque Bera}$   |\n",
    "| ---------------- | --------------------- |  -------------------------------- | ---------------------------- |\n",
    "| OLS              |   0.007    |   0.002    |    74.664   | \n",
    "\n",
    "$R^2$-based statistics close to 0 indicates that almost none of the variance in the dependent variable can be explained by the exogenous factors.\n",
    "The Jarque-Bera test result is far from 0, which means the dependent variable is not normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "\n",
    "Find the beta factors in the FF5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Run both Least Squares and robust regressions on the data, and describe the train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Provide summaries of coefficients and metrics for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Correlation matrix of factor returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we repeat the computation of the correlation matrix between the 5 factors in the Fama-French model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix of factor returns\n",
    "correlation_matrix = df_daily.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix of Factor Returns:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Visualizing the Correlation Matrix of Factor Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Factor Returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heatmap is an effective way to visualize the correlation matrix using colors to represent the correlation coefficients. The color palette of the heatmap shows, warmer colors represent positive correlations and cooler colors represent negative correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairwise relationships in the DataFrame\n",
    "sns.pairplot(df_daily)\n",
    "plt.suptitle('Pairwise Relationships of Factor Returns', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pairplot is used to visualize pairwise relationships between different factors. The grid of scatterplots for each pair of factors, showing their relationships along with histograms for each individual factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clustermap of the correlation matrix\n",
    "sns.clustermap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Clustered Correlation Matrix of Factor Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustermap visually organizes similar factors into clusters based on their correlation coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Covariance Matrix of Factor Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame with factor returns\n",
    "# Assuming df contains the factor returns data\n",
    "\n",
    "# Compute the covariance matrix\n",
    "covariance_matrix = df_daily.cov()\n",
    "\n",
    "# Display the covariance matrix\n",
    "print(\"Covariance Matrix of Factor Returns:\")\n",
    "print(covariance_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Comparison of the Two matrices\n",
    "\n",
    "Correlation and covariance matrix for factors in the Fama-French model during timeframe specified (March 2021 - February 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_correlation and df_covariance are your computed matrices\n",
    "# Create sample correlation and covariance matrices for demonstration\n",
    "df_correlation = pd.DataFrame({\n",
    "    'Mkt-RF': [1.000000, 0.184681, -0.087465, -0.300522, -0.272049, -0.059037],\n",
    "    'SMB': [0.184681, 1.000000, 0.032719, -0.222612, 0.039643, -0.056948],\n",
    "    'HML': [-0.087465, 0.032719, 1.000000, -0.096202, 0.582800, -0.055961],\n",
    "    'RMW': [-0.300522, -0.222612, -0.096202, 1.000000, -0.011507, -0.016484],\n",
    "    'CMA': [-0.272049, 0.039643, 0.582800, -0.011507, 1.000000, -0.005166],\n",
    "    'RF': [-0.059037, -0.056948, -0.055961, -0.016484, -0.005166, 1.000000]\n",
    "}, index=['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF'])\n",
    "\n",
    "df_covariance = pd.DataFrame({\n",
    "    'Mkt-RF': [1.334268,  0.202983, -0.434340, -0.289578, -0.334463,  0.000273],\n",
    "    'SMB': [0.202983,  0.521171,  0.078026, -0.221254, -0.018091,  0.000020],\n",
    "    'HML': [-0.434340,  0.078026,  1.075704,  0.320962,  0.485003, -0.000468],\n",
    "    'RMW': [-0.289578, -0.221254,  0.320962,  0.507100,  0.171608, -0.000283], \n",
    "    'CMA': [-0.334463, -0.018091,  0.485003,  0.171608,  0.383770, -0.000447],\n",
    "    'RF': [0.000273,  0.000020, -0.000468, -0.000283, -0.000447,  0.000081]\n",
    "}, index=['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF'])\n",
    "\n",
    "# Set up the figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot the correlation matrix\n",
    "sns.heatmap(df_correlation, annot=True, cmap='coolwarm', linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title('Correlation Matrix of Factor Returns')\n",
    "\n",
    "# Plot the covariance matrix\n",
    "sns.heatmap(df_covariance, annot=True, cmap='coolwarm', linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title('Covariance Matrix of Factor Returns')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Matrix:\n",
    "The correlation matrix measures the linear relationship between pairs of factors, normalized to a scale of -1 to 1.\n",
    "Values closer to 1 indicate a strong positive linear relationship, while values closer to -1 indicate a strong negative linear relationship.\n",
    "The diagonal elements are always 1, indicating perfect correlation of a factor with itself.\n",
    "Example: The correlation between Mkt-RF and SMB is 0.184681, suggesting a weak positive linear relationship.\n",
    "\n",
    "Covariance Matrix:\n",
    "The covariance matrix measures the extent to which two factors move together, regardless of the scale of their values.\n",
    "Larger values indicate greater variability between the factors, while values closer to zero indicate less variability.\n",
    "The diagonal elements represent the variance of each factor.\n",
    "Example: The covariance between Mkt-RF and HML is -0.43434, indicating a negative covariance (opposite movement)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 \n",
    "Effects of CMA and RMW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable (e.g., 'Mkt-RF') and independent variables (factors)\n",
    "dependent_variable = 'Mkt-RF'\n",
    "independent_variables_all = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "independent_variables_subset = ['Mkt-RF', 'SMB', 'HML', 'RF']  # Subset without CMA and RMW\n",
    "\n",
    "# Function to perform linear regression and return coefficients and model summary\n",
    "def run_regression(df, dependent_variable, independent_variables):\n",
    "    X = df[independent_variables]\n",
    "    X = sm.add_constant(X)\n",
    "    Y = df[dependent_variable]\n",
    "    model = sm.OLS(Y, X)\n",
    "    results = model.fit()\n",
    "    return results\n",
    "\n",
    "# Run regression with all factors (including CMA and RMW)\n",
    "results_all_factors = run_regression(df_daily, dependent_variable, independent_variables_all)\n",
    "\n",
    "# Run regression with subset of factors (excluding CMA and RMW)\n",
    "results_subset_factors = run_regression(df_daily, dependent_variable, independent_variables_subset)\n",
    "\n",
    "# Prepare a summary table to compare results\n",
    "summary_table = pd.DataFrame({\n",
    "    'Factors Included': ['All Factors (CMA & RMW)', 'Subset of Factors'],\n",
    "    'R-squared': [results_all_factors.rsquared, results_subset_factors.rsquared],\n",
    "    'Adjusted R-squared': [results_all_factors.rsquared_adj, results_subset_factors.rsquared_adj],\n",
    "    'Coefficient Mkt-RF': [results_all_factors.params['Mkt-RF'], results_subset_factors.params['Mkt-RF']],\n",
    "    'Coefficient SMB': [results_all_factors.params['SMB'], results_subset_factors.params['SMB']],\n",
    "    'Coefficient HML': [results_all_factors.params['HML'], results_subset_factors.params['HML']],\n",
    "    'Coefficient RF': [results_all_factors.params['RF'], results_subset_factors.params['RF']]\n",
    "})\n",
    "\n",
    "# Display the summary table\n",
    "print(\"Regression Results - Impact of Additional Factors (CMA & RMW):\\n\")\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Markowitz portfolio optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of stock tickers\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "\n",
    "# Download historical stock prices from Yahoo Finance\n",
    "start_date = '2021-03-01'\n",
    "end_date = '2024-02-29'\n",
    "stock_data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Display the first few rows of the stock data\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "returns = stock_data.pct_change().dropna()\n",
    "\n",
    "# Calculate expected returns (mean daily returns)\n",
    "expected_returns = returns.mean()\n",
    "\n",
    "# Calculate covariance matrix of returns\n",
    "covariance_matrix = returns.cov()\n",
    "\n",
    "# Display expected returns and covariance matrix\n",
    "print(\"Expected Returns:\")\n",
    "print(expected_returns)\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(covariance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of stock tickers\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "\n",
    "# Download historical stock prices from Yahoo Finance\n",
    "start_date = '2021-03-01'\n",
    "end_date = '2024-02-29'\n",
    "stock_data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Calculate daily returns\n",
    "returns = stock_data.pct_change().dropna()\n",
    "\n",
    "# Calculate expected returns and covariance matrix\n",
    "expected_returns = returns.mean()\n",
    "covariance_matrix = returns.cov()\n",
    "\n",
    "# Number of assets (stocks)\n",
    "num_assets = len(tickers)\n",
    "\n",
    "# Define the variables (portfolio weights)\n",
    "weights = cp.Variable(num_assets)\n",
    "\n",
    "# Define the risk-free rate (annualized)\n",
    "risk_free_rate = 0.0\n",
    "\n",
    "# Define the target return\n",
    "target_return = 0.10  # Example target return of 10% per year (adjust as needed)\n",
    "\n",
    "# Define the objective function (minimize portfolio volatility)\n",
    "portfolio_variance = cp.quad_form(weights, covariance_matrix.values)\n",
    "objective = cp.Minimize(portfolio_variance)\n",
    "\n",
    "# Define the constraints\n",
    "constraints = [\n",
    "    cp.sum(weights) == 1,  # Fully invested (sum of weights = 1)\n",
    "    expected_returns.values @ weights >= target_return  # Target minimum expected return\n",
    "]\n",
    "\n",
    "# Create the optimization problem\n",
    "optimization_problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve the optimization problem\n",
    "optimization_problem.solve()\n",
    "\n",
    "# Get the optimal asset allocation weights\n",
    "optimal_weights = weights.value\n",
    "\n",
    "# Display optimal asset allocation weights\n",
    "print(\"Optimal Asset Allocation Weights:\")\n",
    "for i, ticker in enumerate(tickers):\n",
    "    print(f\"{ticker}: {optimal_weights[i]:.4f}\")\n",
    "\n",
    "# Calculate and display optimal portfolio expected return and volatility\n",
    "optimal_portfolio_return = expected_returns.values @ optimal_weights\n",
    "optimal_portfolio_volatility = np.sqrt(portfolio_variance.value)\n",
    "print(\"\\nOptimal Portfolio:\")\n",
    "print(f\"Target Return: {target_return:.2%}\")\n",
    "print(f\"Expected Return: {optimal_portfolio_return:.2%}\")\n",
    "print(f\"Volatility (Risk): {optimal_portfolio_volatility:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. Portfolio dependence from factors in FF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fama-French 3-factor model data (Market, SMB, HML)\n",
    "try:\n",
    "    ff_data = yf.download('^GSPC', start=start_date, end=end_date)['Adj Close']\n",
    "    ff_returns = pd.DataFrame(ff_data).pct_change().dropna()\n",
    "    ff_returns = ff_returns.rename(columns={'Adj Close': '^GSPC'})\n",
    "    \n",
    "    # Calculate factor returns (excess returns over risk-free rate)\n",
    "    ff_returns['RF'] = 0.0  # Assuming risk-free rate is zero for simplicity\n",
    "    ff_returns['Mkt-RF'] = ff_returns['^GSPC'] - ff_returns['RF']\n",
    "    ff_returns['SMB'] = returns['AMZN'] - returns['TSLA']  # Example calculation for SMB\n",
    "    ff_returns['HML'] = returns['AAPL'] - returns['MSFT']  # Example calculation for HML \n",
    "    \n",
    "except KeyError:\n",
    "    print(\"Error: S&P 500 index (^GSPC) data not available.\")\n",
    "    # Handle the error gracefully or use an alternative data source\n",
    "# Calculate expected returns and covariance matrix\n",
    "expected_returns = returns.mean()\n",
    "covariance_matrix = returns.cov()\n",
    "\n",
    "# Number of assets (stocks)\n",
    "num_assets = len(tickers)\n",
    "\n",
    "# Define the variables (portfolio weights)\n",
    "weights = cp.Variable(num_assets)\n",
    "\n",
    "# Define the risk-free rate (annualized)\n",
    "risk_free_rate = 0.0\n",
    "\n",
    "# Define the target return\n",
    "target_return = 0.10  # Example target return of 10% per year (adjust as needed)\n",
    "\n",
    "# Define the objective function (minimize portfolio volatility)\n",
    "portfolio_variance = cp.quad_form(weights, covariance_matrix.values)\n",
    "objective = cp.Minimize(portfolio_variance)\n",
    "\n",
    "# Define the constraints\n",
    "constraints = [\n",
    "    cp.sum(weights) == 1,  # Fully invested (sum of weights = 1)\n",
    "    expected_returns.values @ weights >= target_return  # Target minimum expected return\n",
    "]\n",
    "\n",
    "# Create the optimization problem\n",
    "optimization_problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve the optimization problem\n",
    "optimization_problem.solve()\n",
    "\n",
    "# Get the optimal asset allocation weights\n",
    "optimal_weights = weights.value\n",
    "\n",
    "# Calculate and display optimal portfolio expected return and volatility\n",
    "optimal_portfolio_return = expected_returns.values @ optimal_weights\n",
    "optimal_portfolio_volatility = np.sqrt(portfolio_variance.value)\n",
    "print(\"\\nOptimal Portfolio:\")\n",
    "print(f\"Target Return: {target_return:.2%}\")\n",
    "print(f\"Expected Return: {optimal_portfolio_return:.2%}\")\n",
    "print(f\"Volatility (Risk): {optimal_portfolio_volatility:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h. Portfolio dependence from factors in FF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fama-French 5-factor model data (Market, SMB, HML, RMW, CMA)\n",
    "try:\n",
    "    # Calculate RMW, CMA using example calculations\n",
    "    ff_returns['RMW'] = returns['GOOGL'] - returns['MSFT']\n",
    "    ff_returns['CMA'] = returns['AAPL'] - returns['GOOGL']\n",
    "    \n",
    "except KeyError:\n",
    "    print(\"Error: S&P 500 index (^GSPC) data not available.\")\n",
    "    # Handle the error gracefully or use an alternative data source\n",
    "    \n",
    "# Calculate expected returns and covariance matrix\n",
    "expected_returns = returns.mean()\n",
    "covariance_matrix = returns.cov()\n",
    "\n",
    "# Number of assets (stocks)\n",
    "num_assets = len(tickers)\n",
    "\n",
    "# Define the variables (portfolio weights)\n",
    "weights = cp.Variable(num_assets)\n",
    "\n",
    "# Define the risk-free rate (annualized)\n",
    "risk_free_rate = 0.0\n",
    "\n",
    "# Define the target return\n",
    "target_return = 0.10  # Example target return of 10% per year (adjust as needed)\n",
    "\n",
    "# Define the objective function (minimize portfolio volatility)\n",
    "portfolio_variance = cp.quad_form(weights, covariance_matrix.values)\n",
    "objective = cp.Minimize(portfolio_variance)\n",
    "\n",
    "# Define the constraints\n",
    "constraints = [\n",
    "    cp.sum(weights) == 1,  # Fully invested (sum of weights = 1)\n",
    "    expected_returns.values @ weights >= target_return  # Target minimum expected return\n",
    "]\n",
    "\n",
    "# Create the optimization problem\n",
    "optimization_problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve the optimization problem\n",
    "optimization_problem.solve()\n",
    "\n",
    "# Get the optimal asset allocation weights\n",
    "optimal_weights = weights.value\n",
    "\n",
    "# Calculate and display optimal portfolio expected return and volatility\n",
    "optimal_portfolio_return = expected_returns.values @ optimal_weights\n",
    "optimal_portfolio_volatility = np.sqrt(portfolio_variance.value)\n",
    "print(\"\\nOptimal Portfolio:\")\n",
    "print(f\"Target Return: {target_return:.2%}\")\n",
    "print(f\"Expected Return: {optimal_portfolio_return:.2%}\")\n",
    "print(f\"Volatility (Risk): {optimal_portfolio_volatility:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
